"""ä¼˜åŒ–ç‰ˆæµ·æŠ¥çˆ¬å–"""

import requests
from bs4 import BeautifulSoup
import os
import time, random
from datetime import datetime
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
from requests.exceptions import HTTPError
import shutil
import logging

logging.basicConfig(level=logging.DEBUG, format=' %(asctime)s - %(levelname)s - %(message)s')
# logging.disable()

DEFAULT_POSTER_PATH = r"E:\Gazer\DoubanGaze\data\poster"

def compare_date(target_date_1, target_date_2, viewed_date):
    """æ¯”è¾ƒæ¡ç›®æ ‡è®°æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹é—´, åŒ…å«èµ·æ­¢æ—¥æœŸ

    Args:
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        viewed_date (str (YYYY-MM-DD)): çˆ¬å–çš„è§‚çœ‹æ—¥æœŸ

    Returns:
        bool: å¦‚æœåœ¨åœ¨æŒ‡å®šæ—¥æœŸä¹‹é—´, è¿”å› True, å¦åˆ™ False
    """
    try:
        target_date_1_obj = datetime.strptime(target_date_1, "%Y-%m-%d")
        target_date_2_obj = datetime.strptime(target_date_2, "%Y-%m-%d")
        viewed_date_obj = datetime.strptime(viewed_date, "%Y-%m-%d")
        return target_date_1_obj <= viewed_date_obj <= target_date_2_obj
    except ValueError as e:
        logging.error(f"æ—¥æœŸæ ¼å¼é”™: {e}")
        return False
    
def crawl_link(target_link, session, headers):
    """çˆ¬å–ç›®æ ‡é“¾æ¥, è¿”å› soup å¯¹è±¡

    Args:
        target_link (string): ç›®æ ‡é“¾æ¥
        session (requests.Session): requests.Session å¯¹è±¡
        headers (dict): è¯·æ±‚å¤´

    Returns:
        tuple[bs4.BeautifulSoup, int] : soup å¯¹è±¡ å’Œ çŠ¶æ€ç 
    """
    status_code = None
    try:
        target_response = session.get(target_link, headers=headers)
        status_code = target_response.status_code
        target_response.raise_for_status()
        target_soup = BeautifulSoup(target_response.content.decode('utf-8'), 'html.parser')
        return target_soup, status_code
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚é“¾æ¥å¤±è´¥: {e}")
        return None, status_code
    except Exception as e:
       logging.error(f"å…¶ä»–é”™è¯¯: {e}")
       return None, status_code

def create_folder(target_date_1, target_date_2, poster_save_path=DEFAULT_POSTER_PATH):
    """åœ¨æŒ‡å®šç›®å½•ä¸‹åˆ›å»ºè¦ä¿å­˜ <æŒ‡å®šæ—¥æœŸçš„æ¡ç›®æµ·æŠ¥> çš„æ–‡ä»¶å¤¹, 
    åç§°ç”±å¡«å†™çš„èµ·æ­¢æ—¥æœŸå†³å®š, æ ¼å¼ä¸º{target_date_1}_{target_date_2}, 
    e.g. 2024_12_1_2024_12_31
    Args:
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        poster_save_path (string): ä¿å­˜æµ·æŠ¥å›¾ç‰‡çš„æŒ‡å®šç›®å½•
        Defaults to "E:\\Gazer\\DoubanGaze\\data\\poster".

    Returns:
        single_poster_save_path(string): ä¿å­˜æ¯ä¸ªæ¡ç›®æµ·æŠ¥å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ 
        e.g. "E:\\Gazer\\DoubanGaze\\data\\poster\\2024_12_1_2024_12_31".
    """
    date_filename = f"{target_date_1.replace('-', '_')}_{target_date_2.replace('-', '_')}"
    single_poster_save_path = os.path.join(poster_save_path, date_filename)
    os.makedirs(single_poster_save_path, exist_ok=True)
    print(f"å°†ä¿å­˜åœ¨ {single_poster_save_path} ğŸ“‚")
    return single_poster_save_path

def save_poster(poster_src, viewed_date_text, single_poster_save_path, count, headers):
    """ä¿å­˜å•ç‹¬çš„æµ·æŠ¥åˆ°æœ¬åœ°. 

    Args:
        poster_src (string): çˆ¬å–çš„æµ·æŠ¥ä¸‹è½½ URL.
        viewed_date_text (string (YYYY-MM-DD)): å¯¹åº”æ¡ç›®çš„æ ‡è®°æ—¥æœŸ.
        single_poster_save_path (string): create_folder å‡½æ•°è¿”å›çš„
                                    ä¿å­˜æ¯ä¸ªæ¡ç›®æµ·æŠ¥å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ 
        count:  å½“å‰æµ·æŠ¥çš„ä¿å­˜åºå·
        header (dict): è¯·æ±‚å¤´

    Returns:
        bool: ä¿å­˜æˆåŠŸä¸º True, å¤±è´¥ä¸º False
    """
    retry_count = 3  # è®¾ç½®é‡è¯•æ¬¡æ•°
    for attempt in range(retry_count):
        try:
            img_response = requests.get(poster_src, stream=True, headers=headers)
            img_response.raise_for_status()

            # æ„å»ºå›¾ç‰‡æ–‡ä»¶å
            date_filename = viewed_date_text.replace("-", "_")
            img_filename = f"{date_filename}_{count}.jpg" # åŠ å…¥åºå·

            img_path = os.path.join(single_poster_save_path, img_filename)

            # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
            with open(img_path, 'wb') as f:
                shutil.copyfileobj(img_response.raw, f) 
            print(f"img saved {img_path} ğŸ§©")
            return True
        except HTTPError as e:
            logging.error(f"HTTP é”™è¯¯ä»£ç : {e.response.status_code} , å›¾ç‰‡ä¸‹è½½å¤±è´¥: {poster_src}")
            if e.response.status_code == 418 and attempt < retry_count - 1:
               time.sleep(random.randint(5,10))
               logging.debug("U R A TEAPOT, æˆ‘æ­£åœ¨é‡è¯•... â˜•")
               continue 
            else:
              logging.error(f"è¯·æ±‚å¤±è´¥! å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°, å›¾ç‰‡ä¸‹è½½å¤±è´¥: {poster_src}")
              return False 
        except requests.exceptions.RequestException as e:
            logging.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")
            return False
        except Exception as e:
            logging.error(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºç°é”™è¯¯: {e}")
            return False
    logging.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥, è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°, {poster_src}")
    return False 

def get_movie_elements(soup):
    """ä» soup å¯¹è±¡ä¸­è·å–å¹¶è¿”å›æ‰€æœ‰åŒ…å«ç”µå½±æ¡ç›®çš„ div å…ƒç´ 

    Args:
        soup (bs4.BeautifulSoup): åˆå§‹é¡µçš„ soup å¯¹è±¡

    Returns:
        bs4.element.ResultSet: æ‰€æœ‰ç”µå½±æ¡ç›®çš„ div å…ƒç´ åˆ—è¡¨
    """
    return soup.select("#content > div.grid-16-8.clearfix > div.article .item.comment-item")

def get_movie_info(movie_element):
    """ä»å•ä¸ªç”µå½±æ¡ç›® div å…ƒç´ ä¸­è·å–å¹¶è¿”å›è§‚çœ‹æ—¥æœŸå’Œå‹ç¼©æµ·æŠ¥é“¾æ¥

    Args:
        movie_element (bs4.element.Tag): å•ä¸ªç”µå½±æ¡ç›®çš„ div å…ƒç´ 

    Returns:
        tuple[str | None, str | None]: 
        è§‚çœ‹æ—¥æœŸ+å‹ç¼©æµ·æŠ¥é“¾æ¥çš„ Tag å¯¹è±¡å…ƒç»„, æˆ– (None, None)
    """
    viewed_date_element = movie_element.select_one("div.info span.date")
    compressed_link_element = movie_element.select_one("div.pic img")

    if viewed_date_element and compressed_link_element:
        viewed_date_text = viewed_date_element.text.strip()
        compressed_link = compressed_link_element['src']
        return viewed_date_text, compressed_link
    return None, None

def get_headers(cookies, referer):
    return  {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Cookie": cookies,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
        "Referer": referer,
        # "Referer": "https://movie.douban.com/",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Site": "none",
    }

def download_poster_images(cookies, target_date_1, target_date_2, poster_save_path, page_id=1):
    """
    ä»ç»™å®šçš„ URL ä¸­æå–æµ·æŠ¥å›¾ç‰‡é“¾æ¥ï¼Œå¹¶ä¸‹è½½ä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­

    Args:
        cookies: åŒ…å«ç™»å½•ä¿¡æ¯çš„ cookies
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        page_id: çˆ¬å–å¼€å§‹çš„é¡µæ•° Defaults to 1.

    Returns:
        None
    
    Raises:
        requests.exceptions.RequestException: å¦‚æœè¯·æ±‚ç½‘é¡µå¤±è´¥
        Exception: å…¶ä»–å¯èƒ½å‘ç”Ÿçš„é”™è¯¯
    """
    start_time = time.perf_counter() 
    single_poster_save_path = create_folder(target_date_1, target_date_2, poster_save_path)

    session = requests.Session() 
    retry = Retry(connect=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    first_page = True 
    count = 0 
    while True:
        page_processed = (int(page_id) - 1) * 15
        viewed_movie_url = f"https://movie.douban.com/people/133554124/collect?start={page_processed}&sort=time&rating=all&mode=grid&type=all&filter=all"

        headers = get_headers(cookies, viewed_movie_url)

        soup, status_code = crawl_link(viewed_movie_url, session, headers)
        if soup:
            print(f"NOW IN {viewed_movie_url} ğŸ¦")
            try:
                # æ‰“å°å“åº”çš„ HTML ä»¥è°ƒè¯•
                # print(soup.prettify())
                # 1. æ‰¾åˆ°æ‰€æœ‰åŒ…å«ç”µå½±æ¡ç›®çš„ div å…ƒç´ 
                viewed_movie_elements = get_movie_elements(soup)
                logging.debug(f"Found {len(viewed_movie_elements)} marks. FYI: 15.")

                all_items_not_match = True 

                viewed_date_text = "" # åˆå§‹åŒ–
                # 1. éå†æ¯ä¸ªç”µå½±æ¡ç›® div å…ƒç´ 
                for movie_element in viewed_movie_elements:
                    movie_start_time = time.perf_counter() #
                    # 2. è·å–è§‚çœ‹æ—¥æœŸå’Œå‹ç¼©é“¾æ¥
                    viewed_date_text, compressed_link = get_movie_info(movie_element)

                    if viewed_date_text:
                        logging.debug(f"Found date: {viewed_date_text}")
                    
                        if  compare_date(target_date_1, target_date_2, viewed_date_text):
                            all_items_not_match = False # å¦‚æœå‘ç°ç¬¦åˆè¦æ±‚çš„æ¡ç›®ï¼Œåˆ™ä¿®æ”¹æ ‡è®°
                            logging.debug(f"{all_items_not_match}, å³å°†çˆ¬å–...")
                            # 3. æ„é€ æµ·æŠ¥é“¾æ¥
                            photo_id = compressed_link.split("/")[-1].split(".webp")[0][1:]
                            poster_link = f"https://img9.doubanio.com/view/photo/l/public/p{photo_id}.webp"

                            # 4. ä¿®æ”¹ headersï¼Œä½¿ç”¨ compressed_link ä½œä¸º referer
                            headers = get_headers(cookies, compressed_link)

                            # 5. å¢åŠ å»¶è¿Ÿ (é‡è¦ï¼)
                            time.sleep(random.uniform(2, 6)) # éšæœºå»¶è¿Ÿ 5-15 ç§’
                
                            # 4. ä¿å­˜æµ·æŠ¥, è°ƒç”¨ save_posterï¼Œå¹¶ä¼ å…¥ headers
                            count += 1
                            save_poster(poster_link, viewed_date_text, single_poster_save_path, count, headers=headers) 

                    movie_end_time = time.perf_counter() # åœæ­¢å•ä¸ªç”µå½±æ¡ç›®çš„è®¡æ—¶å™¨
                    movie_elapsed_time = movie_end_time - movie_start_time
                    logging.debug(f"å•ä¸ªç”µå½±æ¡ç›®çˆ¬å–è€—æ—¶ï¼š{movie_elapsed_time:.2f} ç§’ â±ï¸")

                # Loop for single page ended here.                        
                # Going to the next page.
                if all_items_not_match and not first_page:
                    print("Done. ğŸ˜º")
                    break
                else:
                    first_page = False
                    page_id += 1
            
            except Exception as e:
                print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        else:
            logging.debug(f"è¯·æ±‚å¤±è´¥! çŠ¶æ€ç : {status_code}")
        
        if not viewed_movie_elements:
            print("No more marks. ğŸ˜º")
            break
    print(f"I have saved {count} posters for U. ğŸ˜¼")
    end_time = time.perf_counter() # åœæ­¢è®¡æ—¶å™¨
    elapsed_time = end_time - start_time
    minutes, seconds = divmod(int(elapsed_time), 60)
    print(f"æ€»è€—æ—¶ï¼š{minutes} åˆ† {seconds} ç§’ ğŸ…") 

if __name__ == "__main__":

    target_date_1 = "2024-1-1"  # TODO å¡«å†™èµ·å§‹æ—¥æœŸ 2025-1-1
    target_date_2 = "2024-12-31" # TODO å¡«å†™æˆªæ­¢æ—¥æœŸ 2025-1-31
    print(f"æ­£åœ¨çˆ¬å– {target_date_1} - {target_date_2} çš„æµ·æŠ¥... ğŸ¤–")
    poster_save_path = r"E:\Gazer\DoubanGaze\data\poster"

    cookies = 'SWEETCOOKIE'
    
    download_poster_images(cookies, target_date_1, target_date_2, poster_save_path, page_id=1) # TODO ä¿®æ”¹èµ·å§‹é¡µ