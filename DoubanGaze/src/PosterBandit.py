import requests
from bs4 import BeautifulSoup
import os
import time, random
from datetime import datetime
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
import shutil
import logging

logging.basicConfig(level=logging.DEBUG, format=' %(asctime)s - %(levelname)s - %(message)s')
# logging.disable()

DEFAULT_POSTER_PATH = r"E:\Gazer\DoubanGaze\data\poster"

def compare_date(target_date_1, target_date_2, viewed_date):
    """æ¯”è¾ƒæ¡ç›®æ ‡è®°æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹é—´, åŒ…å«èµ·æ­¢æ—¥æœŸ

    Args:
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        viewed_date (str (YYYY-MM-DD)): çˆ¬å–çš„è§‚çœ‹æ—¥æœŸ

    Returns:
        bool: å¦‚æœåœ¨åœ¨æŒ‡å®šæ—¥æœŸä¹‹é—´, è¿”å› True, å¦åˆ™ False
    """
    try:
        target_date_1_obj = datetime.strptime(target_date_1, "%Y-%m-%d")
        target_date_2_obj = datetime.strptime(target_date_2, "%Y-%m-%d")
        viewed_date_obj = datetime.strptime(viewed_date, "%Y-%m-%d")
        return target_date_1_obj <= viewed_date_obj <= target_date_2_obj
    except ValueError as e:
        logging.error(f"æ—¥æœŸæ ¼å¼é”™: {e}")
        return False
    
def crawl_link(target_link, session, headers):
    """çˆ¬å–ç›®æ ‡é“¾æ¥, è¿”å› soup å¯¹è±¡

    Args:
        target_link (string): ç›®æ ‡é“¾æ¥
        session (requests.Session): requests.Session å¯¹è±¡
        headers (dict): è¯·æ±‚å¤´

    Returns:
        tuple[bs4.BeautifulSoup, int] : soup å¯¹è±¡ å’Œ çŠ¶æ€ç 
    """
    status_code = None
    try:
        target_response = session.get(target_link, headers=headers)
        status_code = target_response.status_code
        target_response.raise_for_status()
        target_soup = BeautifulSoup(target_response.content.decode('utf-8'), 'html.parser')
        return target_soup, status_code
    except requests.exceptions.RequestException as e:
        logging.error(f"è¯·æ±‚é“¾æ¥å¤±è´¥: {e}")
        return None, status_code
    except Exception as e:
       logging.error(f"å…¶ä»–é”™è¯¯: {e}")
       return None, status_code


def create_folder(target_date_1, target_date_2, poster_save_path=DEFAULT_POSTER_PATH):
    """åœ¨æŒ‡å®šç›®å½•ä¸‹åˆ›å»ºè¦ä¿å­˜ <æŒ‡å®šæ—¥æœŸçš„æ¡ç›®æµ·æŠ¥> çš„æ–‡ä»¶å¤¹, 
    åç§°ç”±å¡«å†™çš„èµ·æ­¢æ—¥æœŸå†³å®š, æ ¼å¼ä¸º{target_date_1}_{target_date_2}, 
    e.g. 2024_12_1_2024_12_31
    Args:
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        poster_save_path (string): ä¿å­˜æµ·æŠ¥å›¾ç‰‡çš„æŒ‡å®šç›®å½•
        Defaults to "E:\\Gazer\\DoubanGaze\\data\\poster".

    Returns:
        single_poster_save_path(string): ä¿å­˜æ¯ä¸ªæ¡ç›®æµ·æŠ¥å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ 
        e.g. "E:\\Gazer\\DoubanGaze\\data\\poster\\2024_12_1_2024_12_31".
    """
    date_filename = f"{target_date_1.replace('-', '_')}_{target_date_2.replace('-', '_')}"
    single_poster_save_path = os.path.join(poster_save_path, date_filename)
    os.makedirs(single_poster_save_path, exist_ok=True)
    return single_poster_save_path

def save_poster(poster_src, viewed_date_text, single_poster_save_path):
    """ä¿å­˜å•ç‹¬çš„æµ·æŠ¥åˆ°æœ¬åœ°. 

    Args:
        poster_src (string): çˆ¬å–çš„æµ·æŠ¥ä¸‹è½½ URL.
        viewed_date_text (string (YYYY-MM-DD)): å¯¹åº”æ¡ç›®çš„æ ‡è®°æ—¥æœŸ.
        single_poster_save_path (string): create_folder å‡½æ•°è¿”å›çš„
                                    ä¿å­˜æ¯ä¸ªæ¡ç›®æµ·æŠ¥å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„ 
    """
    try:
        img_response = requests.get(poster_src, stream=True)
        img_response.raise_for_status()

        # æ„å»ºå›¾ç‰‡æ–‡ä»¶å
        date_filename = viewed_date_text.replace("-", "_")
        img_filename = f"poster_{date_filename}.jpg"

        img_path = os.path.join(single_poster_save_path, img_filename)

        # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
        with open(img_path, 'wb') as f:
            # for chunk in img_response.iter_content(chunk_size=65536):
                # f.write(chunk)
            shutil.copyfileobj(img_response.raw, f) # ç›´æ¥ä½¿ç”¨ shutil.copyfileobj
        print(f"img saved {img_filename}")
    except requests.exceptions.RequestException as e:
        logging.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")
    except Exception as e:
        logging.error(f"ä¿å­˜å›¾ç‰‡æ—¶å‡ºç°é”™è¯¯: {e}")

def get_movie_elements(soup):
    """ä» soup å¯¹è±¡ä¸­è·å–å¹¶è¿”å›æ‰€æœ‰åŒ…å«ç”µå½±æ¡ç›®çš„ div å…ƒç´ 

    Args:
        soup (bs4.BeautifulSoup): åˆå§‹é¡µçš„ soup å¯¹è±¡

    Returns:
        bs4.element.ResultSet: æ‰€æœ‰ç”µå½±æ¡ç›®çš„ div å…ƒç´ åˆ—è¡¨
    """
    return soup.select("#content > div.grid-16-8.clearfix > div.article .item.comment-item")

def get_movie_info(movie_element):
    """ä»å•ä¸ªç”µå½±æ¡ç›® div å…ƒç´ ä¸­è·å–å¹¶è¿”å›è§‚çœ‹æ—¥æœŸå’Œå…·ä½“æ¡ç›®é“¾æ¥

    Args:
        movie_element (bs4.element.Tag): å•ä¸ªç”µå½±æ¡ç›®çš„ div å…ƒç´ 

    Returns:
        tuple[str | None, str | None]: 
        è§‚çœ‹æ—¥æœŸ+å…·ä½“æ¡ç›®é“¾æ¥çš„ Tag å¯¹è±¡å…ƒç»„, æˆ– (None, None)
    """
    viewed_date_element = movie_element.select_one("div.info span.date")
    movie_link_element = movie_element.select_one("div.article div.pic > a")

    if viewed_date_element and movie_link_element:
        viewed_date_text = viewed_date_element.text.strip()
        movie_link = movie_link_element['href']
        return viewed_date_text, movie_link
    return None, None

def get_poster_url(movie_link, session, headers):
    """æ ¹æ®ç”µå½±è¯¦æƒ…é¡µé“¾æ¥è·å–å¹¶è¿”å›æµ·æŠ¥é“¾æ¥

    Args:
        movie_link (str): å…·ä½“æ¡ç›®é“¾æ¥çš„ URL å­—ç¬¦ä¸²
        session (requests.Session): requests.Session å¯¹è±¡
        headers (dict): è¯·æ±‚å¤´

    Returns:
        str | None: æµ·æŠ¥é“¾æ¥çš„ URL å­—ç¬¦ä¸², æˆ– None
    """
    try:
        movie_soup, status_code = crawl_link(movie_link, session, headers)

        if movie_soup:
            poster_img_url_element = movie_soup.select_one("#mainpic > a")
            if poster_img_url_element:
                poster_img_url = poster_img_url_element['href']
                # å¦‚æœä»¥ .webp ç»“å°¾, ç›´æ¥è¿”å›
                if poster_img_url.endswith(".webp"):
                    return poster_img_url
                else:
                    # è¿›å…¥æµ·æŠ¥åˆ—è¡¨
                    poster_img_soup, status_code = crawl_link(poster_img_url, session, headers)

                if poster_img_soup:
                    fst_poster_url_element = poster_img_soup.select_one(
                        "#content div.article div.cover > a"
                        )
                    if fst_poster_url_element:
                        # è¿›å…¥æœ€ç»ˆæµ·æŠ¥
                        poster_final_soup, status_code = crawl_link(fst_poster_url_element['href'], session, headers)

                        if poster_final_soup:
                            logging.debug("æˆåŠŸè¿›å…¥æœ€ç»ˆæµ·æŠ¥")
                            poster_final_url = poster_final_soup.select_one("#content > div > div.article a.mainphoto > img")
                            if poster_final_url:
                                logging.debug("æˆåŠŸæ‰¾åˆ°æœ€ç»ˆæµ·æŠ¥")
                                return poster_final_url['src']
                            else:
                                logging.debug("æ²¡æœ‰æ‰¾åˆ°æœ€ç»ˆæµ·æŠ¥")
                        else:
                            logging.debug("æœªè¿›å…¥æœ€ç»ˆæµ·æŠ¥")
    except requests.exceptions.RequestException as e:
       logging.error(f"è¯·æ±‚é“¾æ¥å¤±è´¥: {e}")
       return None
    return None

def get_headers(cookies, viewed_movie_url):
    return  {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Cookie": cookies,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
        "Referer": viewed_movie_url,
        # "Referer": "https://movie.douban.com/",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-User": "?1",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Site": "none",
    }

def download_poster_images(cookies, target_date_1, target_date_2, poster_save_path, page_id=1):
    """
    ä»ç»™å®šçš„ URL ä¸­æå–æµ·æŠ¥å›¾ç‰‡é“¾æ¥ï¼Œå¹¶ä¸‹è½½ä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸­

    Args:
        cookies: åŒ…å«ç™»å½•ä¿¡æ¯çš„ cookies
        target_date_1 (str (YYYY-MM-DD)): æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        target_date_2 (str (YYYY-MM-DD)): æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
        page_id: çˆ¬å–å¼€å§‹çš„é¡µæ•° Defaults to 1.

    Returns:
        None
    
    Raises:
        requests.exceptions.RequestException: å¦‚æœè¯·æ±‚ç½‘é¡µå¤±è´¥ã€‚
        Exception: å…¶ä»–å¯èƒ½å‘ç”Ÿçš„é”™è¯¯ã€‚
    """
    start_time = time.perf_counter() # å¯åŠ¨è®¡æ—¶å™¨
    single_poster_save_path = create_folder(target_date_1, target_date_2, poster_save_path)

    session = requests.Session() # åœ¨è¿™é‡Œåˆ›å»º session
    retry = Retry(connect=3, backoff_factor=0.5)
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    while True:
        page_processed = (int(page_id) - 1) * 15
        viewed_movie_url = f"https://movie.douban.com/people/122336654/collect?start={page_processed}&sort=time&rating=all&mode=grid&type=all&filter=all"

        headers = get_headers(cookies, viewed_movie_url)

        soup, status_code = crawl_link(viewed_movie_url, session, headers)
        if soup:
            print(f"NOW IN {viewed_movie_url}")
            try:
                # æ‰“å°å“åº”çš„ HTML ä»¥è°ƒè¯•
                # print(soup.prettify())
                # 1. æ‰¾åˆ°æ‰€æœ‰åŒ…å«ç”µå½±æ¡ç›®çš„ div å…ƒç´ 
                viewed_movie_elements = get_movie_elements(soup)
                logging.debug(f"Found {len(viewed_movie_elements)} marks. FYI: 15.")

                count = 0
                viewed_date_text = "" # åˆå§‹åŒ–
                # 1. éå†æ¯ä¸ªç”µå½±æ¡ç›® div å…ƒç´ 
                for movie_element in viewed_movie_elements:
                    movie_start_time = time.perf_counter() # å¯åŠ¨å•ä¸ªç”µå½±æ¡ç›®çš„è®¡æ—¶å™¨
                    # 2. è·å–è§‚çœ‹æ—¥æœŸå’Œé“¾æ¥
                    viewed_date_text, movie_link = get_movie_info(movie_element)

                    if viewed_date_text:
                            logging.debug(f"Found date: {viewed_date_text}")

                            # æ¯”è¾ƒè§‚çœ‹æ—¥æœŸæ˜¯å¦ç¬¦åˆç”¨æˆ·è¾“å…¥, å¦‚æœä¸, break
                            if not compare_date(target_date_1, target_date_2, viewed_date_text):
                                print("Done. ğŸ˜º")
                                break # è·³å‡º for loop

                            # 3. è·å–æµ·æŠ¥é“¾æ¥
                            if movie_link:
                                time.sleep(random.randint(2, 5))
                                fst_poster_url = get_poster_url(movie_link, session, headers) # ä¼ é€’ session
                                if fst_poster_url:
                                    # ä¿å­˜æµ·æŠ¥
                                    save_poster(fst_poster_url, viewed_date_text, single_poster_save_path)
                                    count += 1
                    movie_end_time = time.perf_counter() # åœæ­¢å•ä¸ªç”µå½±æ¡ç›®çš„è®¡æ—¶å™¨
                    movie_elapsed_time = movie_end_time - movie_start_time
                    logging.debug(f"å•ä¸ªç”µå½±æ¡ç›®çˆ¬å–è€—æ—¶ï¼š{movie_elapsed_time:.2f} ç§’")

                # Loop for single page ended here.                        
                print(f"I have saved {count} posters for U. ğŸ˜¼")
                # Going to the next page.
                page_id += 1
            
            except Exception as e:
                print(f"ERROR: {e}")
        else:
            logging.debug(f"è¯·æ±‚å¤±è´¥! çŠ¶æ€ç : {status_code}")
        
        if not viewed_movie_elements or (viewed_date_text and not compare_date(target_date_1, target_date_2, viewed_date_text)):
            break
    end_time = time.perf_counter() # åœæ­¢è®¡æ—¶å™¨
    elapsed_time = end_time - start_time
    minutes, seconds = divmod(int(elapsed_time), 60)
    print(f"æ€»è€—æ—¶ï¼š{minutes} åˆ† {seconds} ç§’") # é¿å…é‡å¤æ ¼å¼åŒ–

if __name__ == "__main__":

    target_date_1 = "2025-1-1"  # TODO å¡«å†™èµ·å§‹æ—¥æœŸ
    target_date_2 = "2025-1-31" # TODO å¡«å†™æˆªæ­¢æ—¥æœŸ
    poster_save_path = r"E:\Gazer\DoubanGaze\data\poster"

    cookies = 'SWEETCOOKIE'
    
    download_poster_images(cookies, target_date_1, target_date_2, poster_save_path, page_id=1)